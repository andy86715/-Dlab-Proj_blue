{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\弘林\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file C:\\Users\\弘林\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n",
      "Building prefix dict from C:\\Users\\弘林\\Documents\\proj-prozac專案\\dict.txt.big.txt ...\n",
      "Loading model from cache C:\\Users\\弘林\\AppData\\Local\\Temp\\jieba.ud3be88558d1ae3c49c32a7e71cdb2d43.cache\n",
      "Loading model cost 2.642 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import xlsxwriter\n",
    "import jieba\n",
    "\n",
    "workbook = xlsxwriter.Workbook('prozac_getdata.xlsx')#create a file \n",
    "worksheet = workbook.add_worksheet()#create a sheet\n",
    "\n",
    "jieba.set_dictionary('dict.txt.big.txt')\n",
    "\n",
    "a = 1561\n",
    "b = 1661\n",
    "e = []\n",
    "f = []\n",
    "g = []\n",
    "h = []\n",
    "n = []\n",
    "m = []\n",
    "o = []\n",
    "def get_title():\n",
    "    for i in range(a,b):\n",
    "        res = requests.get(\"https://www.ptt.cc/bbs/prozac/index\"+str(i)+\".html\")\n",
    "        soup = BeautifulSoup(res.text)\n",
    "        for website in soup.select(\".r-ent\"):\n",
    "            try:    \n",
    "                res2 = requests.get('https://www.ptt.cc/'+website(\"a\")[0]['href'])\n",
    "                soup2 = BeautifulSoup(res2.text)\n",
    "                if \"資訊\" in (soup2.select(\".article-meta-value\")[2].text):\n",
    "                    pass\n",
    "                else:\n",
    "                    c = soup2.select(\".article-meta-value\")[2].text\n",
    "                    d = c.split(\"]\")\n",
    "#標題分類\n",
    "                    e.append(d[0]+\"]\")\n",
    "#標題內容\n",
    "                    f.append(d[1])\n",
    "# 作者\n",
    "                if \"資訊\" in (soup2.select(\".article-meta-value\")[2].text):\n",
    "                    pass\n",
    "                else:\n",
    "                    g.append(soup2.select(\".article-meta-value\")[0].text)\n",
    "# 時間\n",
    "                if \"資訊\" in (soup2.select(\".article-meta-value\")[2].text):\n",
    "                    pass\n",
    "                else:\n",
    "                    h.append(soup2.select(\".article-meta-value\")[3].text)\n",
    "# 文章內容\n",
    "                if \"資訊\" in (soup2.select(\".article-meta-value\")[2].text):\n",
    "                    pass\n",
    "                else:\n",
    "                    soup3 = soup2.select(\".bbs-screen.bbs-content\")[0].text\n",
    "                    content =soup3.split(\"※\") and soup3.split(\"--\")\n",
    "                    content2 = content[0].split(\"2016\")\n",
    "                    n.append(content2[1])\n",
    "# 文章內容結巴\n",
    "                if \"資訊\" in (soup2.select(\".article-meta-value\")[2].text):\n",
    "                    pass\n",
    "                else:\n",
    "                    soup3 = soup2.select(\".bbs-screen.bbs-content\")[0].text\n",
    "                    content =soup3.split(\"※\") and soup3.split(\"--\")\n",
    "                    content2 = content[0].split(\"2016\")\n",
    "                    jeba1=('/'.join(jieba.cut(content2[1],cut_all=False)))\n",
    "                    m.append(jeba1)\n",
    "# 文章網址\n",
    "                if \"資訊\" in (soup2.select(\".article-meta-value\")[2].text):\n",
    "                    pass\n",
    "                else:\n",
    "                    o.append('https://www.ptt.cc/'+website(\"a\")[0]['href'])\n",
    "            except:\n",
    "                pass\n",
    "    row=0\n",
    "    for each_cagetory in e:\n",
    "        worksheet.write(row, 0,each_cagetory)\n",
    "        row = row+1\n",
    "    row=0\n",
    "    for each_title in f:\n",
    "        worksheet.write(row, 1, each_title)\n",
    "        row = row+1\n",
    "    row=0\n",
    "    for each_author in g:\n",
    "        worksheet.write(row, 2, each_author)\n",
    "        row = row+1\n",
    "    row=0\n",
    "    for each_time in h:\n",
    "        worksheet.write(row, 3, each_time)\n",
    "        row = row+1\n",
    "    row=0\n",
    "    for each_content in n:\n",
    "        worksheet.write(row, 4, each_content)\n",
    "        row = row+1\n",
    "    row=0\n",
    "    for each_jieba in m:\n",
    "        worksheet.write(row, 5, each_jieba)\n",
    "        row = row+1\n",
    "    row=0\n",
    "    for each_website in o:\n",
    "        worksheet.write(row, 6, each_website)\n",
    "        row = row+1\n",
    "get_title()\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import xlsxwriter\n",
    "workbook = xlsxwriter.Workbook('prozac_getreply2.xlsx')#create a file \n",
    "worksheet = workbook.add_worksheet()#create a sheet\n",
    "\n",
    "a = 1561\n",
    "b = 1661\n",
    "e = []\n",
    "f = []\n",
    "j = []\n",
    "k = []\n",
    "l = []\n",
    "m = []\n",
    "o = []\n",
    "\n",
    "\n",
    "def get_title():\n",
    "    row=0\n",
    "    row1=0\n",
    "    row2=0\n",
    "    for i in range(a,b):\n",
    "        res = requests.get(\"https://www.ptt.cc/bbs/prozac/index\"+str(i)+\".html\")\n",
    "        soup = BeautifulSoup(res.text)\n",
    "        for website in soup.select(\".r-ent\"):\n",
    "            try:    \n",
    "                res2 = requests.get('https://www.ptt.cc/'+website(\"a\")[0]['href'])\n",
    "                soup2 = BeautifulSoup(res2.text)\n",
    "                if \"資訊\" in (soup2.select(\".article-meta-value\")[2].text):\n",
    "                    pass\n",
    "                else:\n",
    "                    c = soup2.select(\".article-meta-value\")[2].text\n",
    "                    d = c.split(\"]\")\n",
    "#                     標題分類\n",
    "                    e.append(d[0]+\"]\")\n",
    "#                     標題內容\n",
    "                    f.append(d[1])\n",
    "                    o.append('https://www.ptt.cc/'+website(\"a\")[0]['href'])\n",
    "                if \"資訊\" in (soup2.select(\".article-meta-value\")[2].text):\n",
    "                    pass\n",
    "                else:\n",
    "#                     print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#                     推，箭頭，噓\n",
    "                    \n",
    "                    worksheet.write(row1,0,c)\n",
    "                    for each_website in o:\n",
    "                        worksheet.write(row2,1,each_website)\n",
    "                        for i in range(len(soup2.select(\".push\"))):\n",
    "                            j=soup2.select(\".push-tag\")[i].text\n",
    "                            k=soup2.select(\".push-userid\")[i].text\n",
    "                            l=soup2.select(\".push-content\")[i].text\n",
    "                            m=soup2.select(\".push-ipdatetime\")[i].text\n",
    "                            worksheet.write(row, 2, j)\n",
    "    #                    \n",
    "                            worksheet.write(row, 3, k)\n",
    "    #                     \n",
    "                            worksheet.write(row, 4, l)\n",
    "    #                     \n",
    "                            worksheet.write(row, 5, m)\n",
    "                            row=row+1\n",
    "                        row2=row2+row\n",
    "                    row1=row1+row\n",
    "            except:\n",
    "                pass\n",
    "#     row=0\n",
    "#     for each_title in f:\n",
    "#         worksheet.write(row, 1, each_title)\n",
    "#     row=0\n",
    "#     for each_reply_cagetory in j:\n",
    "#         worksheet.write(row, 0, each_reply_cagetory)\n",
    "#         row = row+1\n",
    "#     row=0\n",
    "#     for each_reply_ID in k:\n",
    "#         worksheet.write(row, 1, each_reply_ID)\n",
    "#         row = row+1\n",
    "#     row=0\n",
    "#     for each_reply_content in l:\n",
    "#         worksheet.write(row, 2, each_reply_content)\n",
    "#         row = row+1\n",
    "#     row=0\n",
    "#     for each_reply_time in m:\n",
    "#         worksheet.write(row, 3, each_reply_time)\n",
    "#         row = row+1\n",
    "get_title()\n",
    "workbook.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
